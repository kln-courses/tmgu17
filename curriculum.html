<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Text Mining the Great Unread 2017</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<!-- html --> 
<meta name="src" content="curriculum.tex"> 
<meta name="date" content="2017-05-31 16:05:00"> 
<link rel="stylesheet" type="text/css" href="curriculum.css"> 
</head><body 
>
   <div class="maketitle">
                                                                                             
                                                                                             
                                                                                             
                                                                                             

<h2 class="titleHead">Text Mining the Great Unread 2017</h2>
<div class="author" ></div><br />
<div class="date" ></div>
   </div>
   <h3 class="likesectionHead"><a 
 id="x1-1000"></a>Curriculum</h3>
<!--l. 36--><p class="noindent" ><span 
class="cmbx-10x-x-109">Title</span><br 
class="newline" />Text Mining the Great Unread &#8211; An introduction to data-intensive methods and digital tools for analysis of
texts in the humanities and social sciences<br 
class="newline" />
<!--l. 39--><p class="noindent" ><span 
class="cmbx-10x-x-109">Description of qualifications</span><br 
class="newline" /><span 
class="cmti-10x-x-109">Key learning outcome</span><br 
class="newline" />
     <ol  class="enumerate1" >
     <li 
  class="enumerate" id="x1-1002x1">Demonstrate an ability to delineate and critically evaluate research problems related to text
     analysis in terms of text mining solutions. This involves accessing previous solution to similar
     problems.
     </li>
     <li 
  class="enumerate" id="x1-1004x2">Competences in design and implementation of knowledge discovery pipelines that solve research
     problems related to text analysis. This includes a basic understanding of the various pipeline
     elements and their dependencies (i.e., data selection, preprocessing and transformation followed by
     pattern discovery, data mining and evaluation) as well as implementation in open source software.
     </li>
     <li 
  class="enumerate" id="x1-1006x3">Have an understanding of how to communicate projects and findings in accordance with academic
     and industrial standards.</li></ol>
<!--l. 53--><p class="noindent" ><span 
class="cmti-10x-x-109">Contents</span><br 
class="newline" />
<!--l. 56--><p class="noindent" >Texts have always been essential to research and education in the humanities and social sciences. Close reading
and detailed interpretation have traditionally constituted the standard approach to texts, that is, we combine
qualitative methods and theoretically motivated arguments to a small textual corpus with the purpose of
understanding the meaning of that corpus. However, the rapid expansion of digital fulltext databases,
increasingly faster computers, and advances in language technology are starting to impact the standard
approach by offering a new digital and dataintensive paradigm in the study of text. Humanities
and social science researchers are beginning to ask new types of questions and propose novel
solutions to old problems by using faster and more efficient methods to collect, analyze, and visualize
texts.<br 
class="newline" />Many students (as well as researchers) experience a lack of digital competences when faced with text mining,
that is, the application of tools and methods to analyze large sets of digitized texts. This is unfortunate
                                                                                             
                                                                                             
because text mining 1) enables students to extract high quality information and acquire new knowledge in a
fast and efficient manner; and 2) enhances the qualifications of students for a datadriven job market that is
relying on the very same tools and methods. Finally, many tools and methods in text mining are in
need of a thorough revision by academics who understand the importance of text meaning and
context. Academia and industry alike are therefore in great need of students with text mining
skills.<br 
class="newline" />&#8220;Text Mining the Great Unread&#8221; is an introductory level course to text mining tools and methods in the
humanities and social sciences, which will supply participants with sufficient knowledge and experience
to develop and implement their own text mining projects. The core of the course is a series of
handson workshops supplemented by lectures and tutorials by international researchers and industry
experts. Through the course, participants will become familiar with text mining methods and
software for analyzing and visualizing texts. Participants will learn how to write their own text
mining application in R and Python. Through the workshops, participants will also be presented
with a range of paradigmatic studies and go through explain research design, best practice, and
reporting standards. It is possible to work with one&#8217;s own corpus, but historical and contemporary
corpora (both works of fiction, historical documents and websites) are also available in class.
Participants are not expected to have prior experience with text mining (i.e., programming, statistics, or
visualization).<br 
class="newline" />
<!--l. 83--><p class="noindent" >__________________________________________________________________________________________________________________
<!--l. 85--><p class="noindent" >
   <h4 class="likesubsectionHead"><a 
 id="x1-2000"></a>Program overview</h4>
<!--l. 87--><p class="noindent" ><!--tex4ht:inline--><div class="tabular"> <table id="TBL-2" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-2-1g"><col 
id="TBL-2-1"><col 
id="TBL-2-2"><col 
id="TBL-2-3"><col 
id="TBL-2-4"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-2-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-1"  
class="td11"><span 
class="cmbx-10x-x-109">Week 1</span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-2"  
class="td11">  Time    </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-3"  
class="td11">        Episode             </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-4"  
class="td11">              Reading                      </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-1"  
class="td11">July 24   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-2"  
class="td11">09:00-12:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-3"  
class="td11">      Text analytics         </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-4"  
class="td11">          Fayyad et al. 1996                </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-2"  
class="td11">13:00-15:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-3"  
class="td11"> Computer literacy w. Unix </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-4"  
class="td11">                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-1"  
class="td11">July 25   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-2"  
class="td11">09:00-12:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-3"  
class="td11">Programming w. Python#1</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-4"  
class="td11">           ABSP, chp 7-14                 </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-5-2"  
class="td11">13:00-15:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-5-3"  
class="td11">Programming w. Python#2</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-5-4"  
class="td11">           ABSP, chp 7-14                 </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-6-2"  
class="td11">15:00-17:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-6-3"  
class="td11">    Project session#1       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-6-4"  
class="td11">                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-7-1"  
class="td11">July 26   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-7-2"  
class="td11">09:00-12:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-7-3"  
class="td11">   Processing raw text     </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-7-4"  
class="td11">            NLPP, chp. 3                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-8-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-8-2"  
class="td11">13:00-15:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-8-3"  
class="td11">      Data cleaning         </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-8-4"  
class="td11">            NLPP, chp. 3                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-9-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-9-2"  
class="td11">15:00-17:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-9-3"  
class="td11">        in groups            </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-9-4"  
class="td11">                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-10-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-10-1"  
class="td11">July 27   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-10-2"  
class="td11">09:00-12:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-10-3"  
class="td11">    Word frequencies       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-10-4"  
class="td11">Slingerland et al. 2015, Reagan et al. 2015</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-11-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-11-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-11-2"  
class="td11">13:00-15:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-11-3"  
class="td11"> Ngrams and associations  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-11-4"  
class="td11"> Church et al. 1990, Pechenick et al. 2015 </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-12-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-12-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-12-2"  
class="td11">15:00-17:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-12-3"  
class="td11">        in groups            </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-12-4"  
class="td11">                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-13-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-13-1"  
class="td11">July 28   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-13-2"  
class="td11">09:00-12:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-13-3"  
class="td11">    Project session #2      </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-13-4"  
class="td11">                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-14-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-14-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-14-2"  
class="td11">13:00-15:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-14-3"  
class="td11">      Visualization          </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-14-4"  
class="td11">                                   </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-15-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-15-1"  
class="td11"> </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-15-2"  
class="td11">15:00-17:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-15-3"  
class="td11"> Social event </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-15-4"  
class="td11"></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-16-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-16-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-16-2"  
class="td11">         </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-16-3"  
class="td11">                       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-16-4"  
class="td11">                                   </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-17-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-17-1"  
class="td11"><span 
class="cmbx-10x-x-109">Week 2 </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-17-2"  
class="td11"> Time </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-17-3"  
class="td11"> Episode </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-17-4"  
class="td11"> Reading</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-18-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-18-1"  
class="td11">July 31   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-18-2"  
class="td11">09:00-12:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-18-3"  
class="td11">    Machine learning       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-18-4"  
class="td11">  Brücher el al. 2002, Underwood 2016    </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-19-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-19-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-19-2"  
class="td11">13:00-15:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-19-3"  
class="td11">  Document classification   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-19-4"  
class="td11">            NLPP, chp. 6                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-20-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-20-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-20-2"  
class="td11">15:00-17:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-20-3"  
class="td11">        in groups            </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-20-4"  
class="td11">                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-21-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-21-1"  
class="td11">August 1</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-21-2"  
class="td11">09:00-12:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-21-3"  
class="td11">   Document clustering     </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-21-4"  
class="td11">          Jockers et al. 2010                </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-22-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-22-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-22-2"  
class="td11">13:00-15:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-22-3"  
class="td11">  Latent variable models   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-22-4"  
class="td11">    Blei 2012, Tangherlini et al 2013      </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-23-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-23-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-23-2"  
class="td11">15:00-17:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-23-3"  
class="td11">        in groups            </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-23-4"  
class="td11">                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-24-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-24-1"  
class="td11">August 2</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-24-2"  
class="td11">09:00-12:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-24-3"  
class="td11">    Entity extraction       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-24-4"  
class="td11">        Derczynski et al. 2015             </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-25-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-25-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-25-2"  
class="td11">13:00-15:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-25-3"  
class="td11">       Statistics#1          </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-25-4"  
class="td11">       ISR, chp 4/ISPY, chp. 4            </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-26-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-26-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-26-2"  
class="td11">15:00-17:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-26-3"  
class="td11">    Business analytics      </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-26-4"  
class="td11">                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-27-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-27-1"  
class="td11">August 3</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-27-2"  
class="td11">09:00-12:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-27-3"  
class="td11">       Statistics#2          </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-27-4"  
class="td11">       ISR, chp 6/ISPY, chp. 11           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-28-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-28-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-28-2"  
class="td11">13:00-15:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-28-3"  
class="td11">     Word embedding       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-28-4"  
class="td11">          Mikolov et al. 2013               </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-29-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-29-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-29-2"  
class="td11">15:00-17:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-29-3"  
class="td11">        in group             </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-29-4"  
class="td11">                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-30-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-30-1"  
class="td11">August 4</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-30-2"  
class="td11">09:00-12:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-30-3"  
class="td11">    Project session #3      </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-30-4"  
class="td11">                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-31-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-31-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-31-2"  
class="td11">13:00-15:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-31-3"  
class="td11">    Project session #4      </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-31-4"  
class="td11">                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-32-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-32-1"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-32-2"  
class="td11">15:00-17:00</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-32-3"  
class="td11">       Social event          </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-32-4"  
class="td11">                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-33-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-33-1"  
class="td11">        </td></tr></table></div>
                                                                                             
                                                                                             
   <h4 class="likesubsectionHead"><a 
 id="x1-3000"></a>Textbooks</h4>
<!--l. 130--><p class="noindent" >[ABSP: Introduction to Python]<br 
class="newline" />- Sweigart, A. (2015). Automate the Boring Stuff with Python: Practical Programming for Total Beginners (1
edition). San Francisco: No Starch Press.<br 
class="newline" />
<!--l. 133--><p class="noindent" >[NLPP: Introduction to natural language processing]<br 
class="newline" />- Bird, S., Klein, E., &amp; Loper, E. (2009). Natural Language Processing with Python (1 edition). Beijing;
Cambridge Mass.: O&#8217;Reilly Media.<br 
class="newline" />
<!--l. 136--><p class="noindent" >[ISR: Introduction to statistics &amp; R]*<br 
class="newline" />- Dalgaard, P. (2008). Introductory Statistics with R (2nd edition). New York: Springer.<br 
class="newline" />
<!--l. 139--><p class="noindent" >[ISPY: Introduction to statistics &amp; Python]*<br 
class="newline" />- Haslwanter, T. (2016). An Introduction to Statistics with Python. New York: Springer.<br 
class="newline" />
<!--l. 142--><p class="noindent" ><span 
class="cmr-8">* One book on statistics is sufficient, but depending on your field and personal preferences you might want to use </span><span 
class="cmti-8">R </span><span 
class="cmr-8">instead of</span>
<span 
class="cmr-8">Python for data analysis. If on doubt choose ISPY unless you want to broaden your programming knowledge to multiple</span>
<span 
class="cmr-8">languages.</span><br 
class="newline" />
<!--l. 144--><p class="noindent" >
   <h4 class="likesubsectionHead"><a 
 id="x1-4000"></a>Articles</h4>
     <ul class="itemize1">
     <li class="itemize">Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77&#8211;84.
     </li>
     <li class="itemize">Brücher, H., Knolmayer, G., &amp; Mittermayer, M.-A. (2002). Document classification methods for
     organizing explicit knowledge. Institut fur Wirtschaftsinformatik der Universität Bern.
     </li>
     <li class="itemize">Church,  K.  W.,  &amp;  Hanks,  P.  (1990).  Word  association  norms,  mutual  information,  and
     lexicography. Computational Linguistics, 16(1), 22&#8211;29.
     </li>
     <li class="itemize">Derczynski, L., Maynard, D., Rizzo, G., van Erp, M., Gorrell, G., Troncy, R., &amp; Bontcheva, K.
     (2015). Analysis of named entity recognition and linking for tweets. Information Processing &amp;
     Management, 51(2), 32&#8211;49.
     </li>
     <li class="itemize">Fayyad, U., Piatetsky-Shapiro, G., &amp; Smyth, P. (1996). From data mining to knowledge discovery
     in databases. AI Magazine, 17(3), 37.
     </li>
     <li class="itemize">Jockers, M. L., &amp; Witten, D. M. (2010). A comparative study of machine learning methods for
     authorship attribution. Literary and Linguistic Computing, fqq001.
     </li>
     <li class="itemize">Mikolov, Tomas; Sutskever, Ilya; Chen, Kai; Corrado, Greg; Dean, Jeffrey (2013). &#8220;Distributed
     Representations of Words and Phrases and their Compositionality&#8221;. arXiv:1310.4546.
     </li>
     <li class="itemize">Pechenick, E. A., Danforth, C. M., &amp; Dodds, P. S. (2015). Characterizing the Google Books
     corpus: strong limits to inferences of socio-cultural and linguistic evolution. PloS One, 10(10),
     e0137041.
                                                                                             
                                                                                             
     </li>
     <li class="itemize">Reagan, A., Tivnan, B., Williams, J. R., Danforth, C. M., &amp; Dodds, P. S. (2015). Benchmarking
     sentiment analysis methods for large-scale texts: A case for using continuum-scored words and
     word shift graphs. arXiv Preprint arXiv:1512.00531.
     </li>
     <li class="itemize">Slingerland, E., &amp; Chudek, M. (2011). The Prevalence of Mind-Body Dualism in Early China.
     Cognitive Science, 35(5), 997&#8211;1007.
     </li>
     <li class="itemize">Tangherlini, T. R., &amp; Leonard, P. (2013). Trawling in the Sea of the Great Unread: Sub-corpus
     topic modeling and Humanities research. Poetics, 41(6), 725&#8211;749.
     </li>
     <li class="itemize">Underwood, T. (2016). The Life Cycles of Genres. [Machine learning, Classification] <br 
class="newline" />Retrieved from https://www.ideals.illinois.edu/handle/2142/90161.</li></ul>
<!--l. 174--><p class="noindent" >
   <h4 class="likesubsectionHead"><a 
 id="x1-5000"></a>Preparation Details</h4>
     <ul class="itemize1">
     <li class="itemize">Answer pre-workshop questionnaire (you receive invitation by email)
     </li>
     <li class="itemize">Read Part 1 in Automate the Boring Stuff with Python
     </li>
     <li class="itemize">Read Chapter 1 in Natural Language Processing with Python
     </li>
     <li class="itemize">Install Anaconda distribution of Python: <a 
href="https://www.continuum.io/downloads" class="url" ><span 
class="cmtt-10x-x-109">https://www.continuum.io/downloads</span></a>. If in doubt
     choose the Python 2.7 version.
     </li>
     <li class="itemize">If    you    want    to    do    statistics    in    <span 
class="cmti-10x-x-109">R    </span>(or    are    simply    interested)    install    <span 
class="cmti-10x-x-109">R</span>:
     <a 
href="https://www.r-project.org/" class="url" ><span 
class="cmtt-10x-x-109">https://www.r-_project.org/</span></a>
     </li>
     <li class="itemize">Install   Git:   <a 
href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git" class="url" ><span 
class="cmtt-10x-x-109">https://git-_scm.com/book/en/v2/Getting-_Started-_Installing-_Git</span></a>.   If
     your operating system is Windows use: <a 
href="https://git-for-windows.github.io/" class="url" ><span 
class="cmtt-10x-x-109">https://git-_for-_windows.github.io/</span></a>
     </li>
     <li class="itemize">Make free GitHub account at <a 
href="https://github.com/" class="url" ><span 
class="cmtt-10x-x-109">https://github.com/</span></a></li></ul>
<div class="center" 
>
<!--l. 185--><p class="noindent" >
<!--l. 186--><p class="noindent" ><span 
class="cmti-10x-x-109">Should you have any specific requests, please contact: </span><a 
href="mailto:kln@cas.au.dk" >kln@cas.au.dk</a>.<br />
</div>
<!--l. 189--><p class="noindent" >__________________________________________________________________________________________________________________
                                                                                             
                                                                                             
<!--l. 191--><p class="noindent" >
   <h4 class="likesubsectionHead"><a 
 id="x1-6000"></a>Description of Episodes</h4>
<!--l. 192--><p class="noindent" >Almost all episodes are interactive workshops consisting of short introductory presentations followed by
tutorials and exercises. You will all develop projects in groups that you work on during project sessions and in
group episodes. Supervision is mandatory and available during in group episodes.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-7000"></a>Text analytics</h5>
<!--l. 194--><p class="noindent" >Text mining (<span 
class="cmsy-10x-x-109">&sim; </span>text analytics) is a heterogeneous research field that focuses on extraction of meaningful
patterns from unstructured and text-heavy data. The meaningful patterns are typically extracted by applying
statistical learning (i.e., machine learning) to target data sets from large non-relational databases. This
introductory lecture introduces data-intensive knowledge discovery in fulltext databases for humanities and
social sciences and we focus on mapping the research possibilites available in the various domains of text
analytics.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-8000"></a>Computer literacy with Unix</h5>
<!--l. 196--><p class="noindent" >In order to understand what computers can contribute, we practice how to think algorithmically with Unix
shell scripting. A shell script is a computer program the is designed to run from a Unix command-line
interface. Beyond training computational literacy, scripting in Unix-like systems turns is very handy for
managing files.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-9000"></a>Programming with Python#1</h5>
<!--l. 198--><p class="noindent" >Introduction to Python basics in Jupyter Notebook. This episode introduce file manipulation, data types, flow
control with booleans and conditional statements, and import of modules.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-10000"></a>Programming with Python#2</h5>
<!--l. 200--><p class="noindent" >Part 2 of the introduction to python basics which now transitions to the Spyder IDE. This episode introduces
functions, classes, errors and debugging. Will will pay particular focus on defensive programming and how to
write robust programs that run from the shell.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-11000"></a>Processing raw text</h5>
<!--l. 202--><p class="noindent" >How do we extract the relevant data from existing fulltext databases, urls, or even badly scanned
PDF files? In this session we will develop principles for building a data set, extract text from
websites and documents, and do simple preprocessing steps such as tokenization and stopword
removal.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-12000"></a>Data preparation</h5>
<!--l. 204--><p class="noindent" >This episode introduces regular expressions for data clarning and information extraction, text normalization
with stemming and lemmatization, and bag-of-words models and vector space representations for documents
(term-document matricies).
   <h5 class="likesubsubsectionHead"><a 
 id="x1-13000"></a>Word frequencies</h5>
<!--l. 206--><p class="noindent" >Having prepared our data set in the previous episodes, we now turn to basic statistical properties of natural
language reflected in word frequency distributions. We will explore a simple application that have gained huge
popularity in recent years, namely, dictionary-based sentiment analysis that can be used to estimate affective
content of texts.
                                                                                             
                                                                                             
   <h5 class="likesubsubsectionHead"><a 
 id="x1-14000"></a>Ngrams and associations</h5>
<!--l. 208--><p class="noindent" >Because words meaning depends critically on their context, this episode will introduce several techniques to
model word context and word associations. We will look at word collocations, co-occurrence matrix and
similarity-based measures for vector spaces.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-15000"></a>Visualization</h5>
<!--l. 210--><p class="noindent" >Visual inpspection of data is a valuable tools both for discovering patterns and communicating results. In this
episode we will work with the matplotlib and seaborn libraries in order to check our data integrity, compare
patterns and visualize findings.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-16000"></a>Machine learning</h5>
<!--l. 212--><p class="noindent" >Machine learning is &#8216;the new black&#8217; in data analysis especially when we deal with large data sets with a low of
noise. Instead of classical analysis, we can train models to discover patterns in our data set and
generalize to unseen cases. This episode is a lecture on application of machine learning in text
analytics.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-17000"></a>Document classification</h5>
<!--l. 214--><p class="noindent" >Classification is a supervised learning task, which is one of the central tasks in machine learning. In this
episode we will train several types of supervised learning algorithms to classify a set of documents according to
some classes or labels (e.g., genre or author features).
   <h5 class="likesubsubsectionHead"><a 
 id="x1-18000"></a>Document clustering</h5>
<!--l. 216--><p class="noindent" >While supervised learning assumes labeled data, clustering, which is an unsupervised learning task, can be
used to discover meaningful classes in the data. In this episode we will use hard and hierarchical clustering to
partition our data sets.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-19000"></a>Latent variable models</h5>
<!--l. 218--><p class="noindent" >Latent variable models cover both geometrical and probabilistic approached to modelling latent
themes or semantic structures in collections of documents. In this episode we will cover both latent
semantic analysis (LSA) and latent Dirichlet allocation (LDA) also called topic modeling in digital
humanities.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-20000"></a>Entity extraction</h5>
<!--l. 220--><p class="noindent" >Named entities are unique identifiers for entities in texts (a proper noun serving as a name for someone or
something). We can use entity extraction to detect proper nouns in texts and classify them into categories such
as person, localization and organization.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-21000"></a>Statistics#1</h5>
<!--l. 222--><p class="noindent" >Introduction to descriptive statistics and graphics in R. Before going into the actual statistical modelling and
analysis of a text data set, it is often useful to make some simple characterizations of the data interms of
summary statistics and graphics.
                                                                                             
                                                                                             
   <h5 class="likesubsubsectionHead"><a 
 id="x1-22000"></a>Business analytics</h5>
<!--l. 225--><p class="noindent" >One of AU&#8217;s leading specialists in Business Analytics will explain how we can use text data to explore and
investigate business performance in order to gain insight and drive business planning. This episode is a
lecture.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-23000"></a>Word embedding</h5>
<!--l. 227--><p class="noindent" >Word embedding algorithms have become popular tools for uncovering the full associative structure of large
text collections. Using an artificial neural network, we can construct high-dimensional vector representations of
words, sentences, and documents and explore their similarity structure.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-24000"></a>Statistics#2</h5>
<!--l. 229--><p class="noindent" >A look at simple statistical tests for comparing data between two classes or a prior stipulated class and
techniques for predicting variation in a response variable based on one and multiple explanatory
variables.
<!--l. 233--><p class="noindent" >__________________________________________________________________________________________________________________
<!--l. 236--><p class="noindent" ><span 
class="cmbx-10x-x-109">Kristoffer L. Nielbo </span><br 
class="newline" />Associate Professor, MA, PhD, <br 
class="newline" />Interacting Minds Centre, Department of Culture and Society.<br 
class="newline" />Jens Chr. Skous Vej 4, building 1483, 326, 8000 Aarhus C., DK.<br 
class="newline" />Email: <span 
class="cmbx-10x-x-109">kln@cas.au.dk </span><br 
class="newline" />Phone: <span 
class="cmbx-10x-x-109">+45 8716 2903 </span><br 
class="newline" />Mobile: <span 
class="cmbx-10x-x-109">+45 2683 2608 </span><br 
class="newline" />
    
</body></html> 

                                                                                             


